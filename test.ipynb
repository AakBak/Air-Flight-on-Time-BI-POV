{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import functions as F\n",
    "from numerize.numerize import numerize\n",
    "import functions as f\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "from st_pages import Page, show_pages, Section\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession with appropriate settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AirlineDelays\") \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Define the schema\n",
    "schema_1991 = StructType([\n",
    "    StructField('Month', IntegerType(), True),\n",
    "    StructField('DayofMonth', IntegerType(), True),\n",
    "    StructField('DayOfWeek', IntegerType(), True),\n",
    "    StructField('DepTime', DoubleType(), True),\n",
    "    StructField('UniqueCarrier', StringType(), True),\n",
    "    StructField('FlightNum', IntegerType(), True),\n",
    "    StructField('ActualElapsedTime', DoubleType(), True),\n",
    "    StructField('ArrDelay', DoubleType(), True),\n",
    "    StructField('DepDelay', DoubleType(), True),\n",
    "    StructField('Origin', StringType(), True),\n",
    "    StructField('Dest', StringType(), True),\n",
    "    StructField('Distance', DoubleType(), True),\n",
    "    StructField('Cancelled', IntegerType(), True),\n",
    "    StructField('Diverted', IntegerType(), True),\n",
    "])\n",
    "\n",
    "schema_2001 = StructType([\n",
    "    StructField('Month', IntegerType(), True),\n",
    "    StructField('DayofMonth', IntegerType(), True),\n",
    "    StructField('DayOfWeek', IntegerType(), True),\n",
    "    StructField('DepTime', DoubleType(), True),\n",
    "    StructField('UniqueCarrier', StringType(), True),\n",
    "    StructField('FlightNum', IntegerType(), True),\n",
    "    StructField('TailNum', StringType(), True),\n",
    "    StructField('ActualElapsedTime', DoubleType(), True),\n",
    "    StructField('AirTime', DoubleType(), True),\n",
    "    StructField('ArrDelay', DoubleType(), True),\n",
    "    StructField('DepDelay', DoubleType(), True),\n",
    "    StructField('Origin', StringType(), True),\n",
    "    StructField('Dest', StringType(), True),\n",
    "    StructField('Distance', IntegerType(), True),\n",
    "    StructField('TaxiIn', IntegerType(), True),\n",
    "    StructField('TaxiOut', IntegerType(), True),\n",
    "    StructField('Cancelled', IntegerType(), True),\n",
    "    StructField('Diverted', IntegerType(), True),\n",
    "])\n",
    "\n",
    "def load_data():\n",
    "    # Read the CSV files into PySpark DataFrames\n",
    "    df1 = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .schema(schema_1991) \\\n",
    "        .load('1991_cleaned.csv.gz')\n",
    "\n",
    "    df2 = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .schema(schema_2001) \\\n",
    "        .load('2001_cleaned.csv.gz')\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "# Load data\n",
    "df1, df2 = load_data()\n",
    "\n",
    "df1_agg = df1.withColumn('DELAYED', when(df1['ArrDelay'] <= 0, 0).otherwise(1))\n",
    "df2_agg = df2.withColumn('DELAYED', when(df2['ArrDelay'] <= 0, 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+-------------+---------+-----------------+--------+--------+------+----+--------+---------+--------+-------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|UniqueCarrier|FlightNum|ActualElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|Cancelled|Diverted|DELAYED|\n",
      "+-----+----------+---------+-------+-------------+---------+-----------------+--------+--------+------+----+--------+---------+--------+-------+\n",
      "|    1|         1|        2| 1709.0|           US|      112|            155.0|     0.0|     4.0|   TPA| SYR|  1104.0|        0|       0|      0|\n",
      "|    1|         2|        3| 1704.0|           US|      112|            162.0|     2.0|    -1.0|   TPA| SYR|  1104.0|        0|       0|      1|\n",
      "|    1|         3|        4| 1705.0|           US|      112|            149.0|   -10.0|     0.0|   TPA| SYR|  1104.0|        0|       0|      0|\n",
      "|    1|         4|        5| 1709.0|           US|      112|            162.0|     7.0|     4.0|   TPA| SYR|  1104.0|        0|       0|      1|\n",
      "|    1|         5|        6| 1703.0|           US|      112|            153.0|    -8.0|    -2.0|   TPA| SYR|  1104.0|        0|       0|      0|\n",
      "|    1|         6|        7| 1711.0|           US|      112|            158.0|     5.0|     6.0|   TPA| SYR|  1104.0|        0|       0|      1|\n",
      "|    1|         7|        1| 1711.0|           US|      112|            155.0|     2.0|     6.0|   TPA| SYR|  1104.0|        0|       0|      1|\n",
      "|    1|         8|        2| 1704.0|           US|      112|            150.0|   -10.0|    -1.0|   TPA| SYR|  1104.0|        0|       0|      0|\n",
      "|    1|         9|        3| 1805.0|           US|      112|            157.0|    58.0|    60.0|   TPA| SYR|  1104.0|        0|       0|      1|\n",
      "|    1|        10|        4| 1703.0|           US|      112|            163.0|     2.0|    -2.0|   TPA| SYR|  1104.0|        0|       0|      1|\n",
      "+-----+----------+---------+-------+-------------+---------+-----------------+--------+--------+------+----+--------+---------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_agg.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, categorical_cols, numerical_cols):\n",
    "    # Remove null values\n",
    "    all_columns = list()\n",
    "    for col_name in df.columns:\n",
    "        all_columns.append(col_name)\n",
    "        df = df.filter(col(col_name).isNotNull())\n",
    "\n",
    "    # Indexing and One-Hot Encoding for categorical columns\n",
    "    indexers = [\n",
    "        StringIndexer(inputCol=col_name, outputCol=col_name + \"_index\", handleInvalid=\"keep\")\n",
    "        for col_name in categorical_cols\n",
    "    ]\n",
    "    # encoders = [\n",
    "    #     OneHotEncoder(inputCol=col_name + \"_index\", outputCol=col_name + \"_encoded\")\n",
    "    #     for col_name in categorical_cols\n",
    "    # ]\n",
    "\n",
    "    # Assemble all encoded categorical columns and numerical columns into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[col_name + \"_index\" for col_name in categorical_cols] + numerical_cols,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Assemble all stages into a pipeline\n",
    "    stages = indexers + [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    # Fit the pipeline to the DataFrame\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "\n",
    "    # Transform the DataFrame using the fitted pipeline\n",
    "    transformed_df = pipeline_model.transform(df)\n",
    "\n",
    "    # transformed_df = transformed_df.drop(*[col_name + \"_index\" for col_name in categorical_cols] + [col_name + \"_encoded\" for col_name in categorical_cols])\n",
    "    transformed_df = transformed_df.drop(*[col_name + \"_index\" for col_name in categorical_cols])\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+-------------+---------+-----------------+--------+--------+------+----+--------+---------+--------+-------+-------------------------------------------------------------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|UniqueCarrier|FlightNum|ActualElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|Cancelled|Diverted|DELAYED|features                                                           |\n",
      "+-----+----------+---------+-------+-------------+---------+-----------------+--------+--------+------+----+--------+---------+--------+-------+-------------------------------------------------------------------+\n",
      "|1    |1         |2        |1709.0 |US           |112      |155.0            |0.0     |4.0     |TPA   |SYR |1104.0  |0        |0       |0      |[0.0,30.0,58.0,1.0,1.0,2.0,1709.0,112.0,155.0,4.0,1104.0,0.0,0.0]  |\n",
      "|1    |2         |3        |1704.0 |US           |112      |162.0            |2.0     |-1.0    |TPA   |SYR |1104.0  |0        |0       |1      |[0.0,30.0,58.0,1.0,2.0,3.0,1704.0,112.0,162.0,-1.0,1104.0,0.0,0.0] |\n",
      "|1    |3         |4        |1705.0 |US           |112      |149.0            |-10.0   |0.0     |TPA   |SYR |1104.0  |0        |0       |0      |[0.0,30.0,58.0,1.0,3.0,4.0,1705.0,112.0,149.0,0.0,1104.0,0.0,0.0]  |\n",
      "|1    |4         |5        |1709.0 |US           |112      |162.0            |7.0     |4.0     |TPA   |SYR |1104.0  |0        |0       |1      |[0.0,30.0,58.0,1.0,4.0,5.0,1709.0,112.0,162.0,4.0,1104.0,0.0,0.0]  |\n",
      "|1    |5         |6        |1703.0 |US           |112      |153.0            |-8.0    |-2.0    |TPA   |SYR |1104.0  |0        |0       |0      |[0.0,30.0,58.0,1.0,5.0,6.0,1703.0,112.0,153.0,-2.0,1104.0,0.0,0.0] |\n",
      "|1    |6         |7        |1711.0 |US           |112      |158.0            |5.0     |6.0     |TPA   |SYR |1104.0  |0        |0       |1      |[0.0,30.0,58.0,1.0,6.0,7.0,1711.0,112.0,158.0,6.0,1104.0,0.0,0.0]  |\n",
      "|1    |7         |1        |1711.0 |US           |112      |155.0            |2.0     |6.0     |TPA   |SYR |1104.0  |0        |0       |1      |[0.0,30.0,58.0,1.0,7.0,1.0,1711.0,112.0,155.0,6.0,1104.0,0.0,0.0]  |\n",
      "|1    |8         |2        |1704.0 |US           |112      |150.0            |-10.0   |-1.0    |TPA   |SYR |1104.0  |0        |0       |0      |[0.0,30.0,58.0,1.0,8.0,2.0,1704.0,112.0,150.0,-1.0,1104.0,0.0,0.0] |\n",
      "|1    |9         |3        |1805.0 |US           |112      |157.0            |58.0    |60.0    |TPA   |SYR |1104.0  |0        |0       |1      |[0.0,30.0,58.0,1.0,9.0,3.0,1805.0,112.0,157.0,60.0,1104.0,0.0,0.0] |\n",
      "|1    |10        |4        |1703.0 |US           |112      |163.0            |2.0     |-2.0    |TPA   |SYR |1104.0  |0        |0       |1      |[0.0,30.0,58.0,1.0,10.0,4.0,1703.0,112.0,163.0,-2.0,1104.0,0.0,0.0]|\n",
      "+-----+----------+---------+-------+-------------+---------+-----------------+--------+--------+------+----+--------+---------+--------+-------+-------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your PySpark DataFrame and categorical_cols is a list of categorical column names\n",
    "transformed_df = preprocess_data(df1_agg, categorical_cols=[\"UniqueCarrier\", \"Origin\", \"Dest\"],\n",
    "                                     numerical_cols=['Month','DayofMonth','DayOfWeek','DepTime','FlightNum','ActualElapsedTime','DepDelay','Distance','Cancelled','Diverted'])\n",
    "transformed_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df1991 = transformed_df.sample(fraction=0.002, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_data, test_data = sample_df1991.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 1 has 233 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m GBT_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[GBT])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train the GBT model on the `train_data` DataFrame\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m GBT_model \u001b[38;5;241m=\u001b[39m \u001b[43mGBT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Apps\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32md:\\Apps\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32md:\\Apps\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Apps\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32md:\\Apps\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 1 has 233 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "# Create a Gradient-Boosted Tree classifier with the label column set to \"DELAY\" and the features column set to \"FEATURES\"\n",
    "GBT = GBTClassifier(labelCol=\"DELAYED\", featuresCol=\"features\")\n",
    "\n",
    "# Define the pipeline\n",
    "GBT_pipeline = Pipeline(stages=[GBT])\n",
    "\n",
    "# Train the GBT model on the `train_data` DataFrame\n",
    "GBT_model = GBT.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "feature_importance = GBT_model.stages[-1].featureImportances\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Feature Importance:\")\n",
    "for i, importance in enumerate(feature_importance.toArray()):\n",
    "    print(f\"Feature {i}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained GBT model to make predictions on the `test_data` DataFrame\n",
    "GBT_predictions = GBT_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation for accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the evaluator\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"DELAYED\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_evaluator.evaluate(GBT_predictions)\n",
    "\n",
    "# Convert to percentage\n",
    "percentage_accuracy = accuracy * 100\n",
    "\n",
    "print(f\"Accuracy: {percentage_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Assuming 'DELAYED' is the target variable\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"DELAYED\", numTrees=100)\n",
    "\n",
    "# Define the pipeline\n",
    "# rf_pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "model = rf.fit(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances as Python floats\n",
    "feature_importance = [float(importance) for importance in feature_importances]\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "feature_importance_df = spark.createDataFrame([(feature, importance) for feature, importance in zip(transformed_df.columns, feature_importance)],\n",
    "                                               [\"Feature\", \"Importance\"])\n",
    "\n",
    "# Display the feature importances\n",
    "feature_importance_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
