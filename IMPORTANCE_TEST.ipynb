{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Step 1: Load datasets and create DELAYED column\n",
    "df_1991 = pd.read_csv('1991_cleaned.csv.gz').sample(10000,random_state=42)\n",
    "df_2001 = pd.read_csv('2001_cleaned.csv.gz').sample(10000,random_state=42)\n",
    "\n",
    "df_1991['DELAYED'] = (df_1991['ArrDelay'] > 0).astype(int)\n",
    "df_2001['DELAYED'] = (df_2001['ArrDelay'] > 0).astype(int)\n",
    "\n",
    "df_1991 = df_1991.drop('ArrDelay', axis=1)\n",
    "df_2001 = df_2001.drop('ArrDelay', axis=1)\n",
    "\n",
    "# Step 2: Feature engineering\n",
    "# Selecting relevant columns\n",
    "cat_cols = [\"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
    "num_cols_1991 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'DepDelay', 'Distance',\n",
    "                 'Cancelled', 'Diverted']\n",
    "num_cols_2001 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'AirTime', 'DepDelay',\n",
    "                 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted']\n",
    "\n",
    "# Step 3: Create transformers for numerical and categorical columns\n",
    "num_transformer = StandardScaler()\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor_1991 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_1991),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "preprocessor_2001 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_2001),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Step 4: Train and evaluate ML models\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_1991, X_test_1991, y_train_1991, y_test_1991 = train_test_split(\n",
    "    df_1991.drop('DELAYED', axis=1), df_1991['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_2001, X_test_2001, y_train_2001, y_test_2001 = train_test_split(\n",
    "    df_2001.drop('DELAYED', axis=1), df_2001['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    # 'Random Forest': RandomForestClassifier(),\n",
    "    # 'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Create empty DataFrames for accuracies and feature importance\n",
    "accuracy_table_1991 = pd.DataFrame(index=['Accuracy'])\n",
    "feature_importance_table_1991 = pd.DataFrame()\n",
    "\n",
    "accuracy_table_2001 = pd.DataFrame(index=['Accuracy'])\n",
    "feature_importance_table_2001 = pd.DataFrame()\n",
    "\n",
    "# Create folders if they don't exist\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "# os.makedirs(\"feature_importance\", exist_ok=True)\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model in models.items():\n",
    "    # Training and evaluating on 1991 data\n",
    "    model_pipe_1991 = make_pipeline(preprocessor_1991, model)\n",
    "    accuracy_1991 = train_and_evaluate_model(model_pipe_1991, X_train_1991, y_train_1991, X_test_1991, y_test_1991)\n",
    "    accuracy_table_1991[model_name] = [f\"{accuracy_1991 * 100:.3f}%\"]\n",
    "\n",
    "    # # Save the model for 1991\n",
    "    # model_filename_1991 = f\"models/{model_name}_1991_model.joblib\"\n",
    "    # joblib.dump(model_pipe_1991, model_filename_1991)\n",
    "\n",
    "    # Get feature importance for 1991\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance_1991 = model.feature_importances_\n",
    "        features_1991 = X_train_1991.columns\n",
    "        importance_dict_1991 = dict(zip(features_1991, feature_importance_1991))\n",
    "        feature_importance_table_1991[model_name] = pd.Series(importance_dict_1991).apply(lambda x: f\"{x * 100:.3f}%\")\n",
    "\n",
    "        # # Save feature importances for 1991 as CSV\n",
    "        # feature_importance_filename_1991 = f\"feature_importance/{model_name}_1991_feature_importance.csv\"\n",
    "        # pd.DataFrame(importance_dict_1991.items(), columns=['Feature', 'Importance']).to_csv(feature_importance_filename_1991, index=False)\n",
    "\n",
    "    # Training and evaluating on 2001 data\n",
    "    model_pipe_2001 = make_pipeline(preprocessor_2001, model)\n",
    "    accuracy_2001 = train_and_evaluate_model(model_pipe_2001, X_train_2001, y_train_2001, X_test_2001, y_test_2001)\n",
    "    accuracy_table_2001[model_name] = [f\"{accuracy_2001 * 100:.3f}%\"]\n",
    "\n",
    "    # # Save the model for 2001\n",
    "    # model_filename_2001 = f\"models/{model_name}_2001_model.joblib\"\n",
    "    # joblib.dump(model_pipe_2001, model_filename_2001)\n",
    "\n",
    "    # Get feature importance for 2001\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance_2001 = model.feature_importances_\n",
    "        features_2001 = X_train_2001.columns\n",
    "        importance_dict_2001 = dict(zip(features_2001, feature_importance_2001))\n",
    "        feature_importance_table_2001[model_name] = pd.Series(importance_dict_2001).apply(lambda x: f\"{x * 100:.3f}%\")\n",
    "\n",
    "        # # Save feature importances for 2001 as CSV\n",
    "        # feature_importance_filename_2001 = f\"feature_importance/{model_name}_2001_feature_importance.csv\"\n",
    "        # pd.DataFrame(importance_dict_2001.items(), columns=['Feature', 'Importance']).to_csv(feature_importance_filename_2001, index=False)\n",
    "\n",
    "# # Save accuracy tables to CSV files\n",
    "# accuracy_table_1991.to_csv(\"accuracy_table_1991.csv\")\n",
    "# accuracy_table_2001.to_csv(\"accuracy_table_2001.csv\")\n",
    "\n",
    "# Display the tables\n",
    "print(\"Accuracy Table for 1991 Dataset:\")\n",
    "print(accuracy_table_1991)\n",
    "\n",
    "print(\"\\nAccuracy Table for 2001 Dataset:\")\n",
    "print(accuracy_table_2001)\n",
    "\n",
    "print(\"\\nFeature Importance Table for 1991 Dataset:\")\n",
    "print(feature_importance_table_1991)\n",
    "\n",
    "print(\"\\nFeature Importance Table for 2001 Dataset:\")\n",
    "print(feature_importance_table_2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer for LabelEncoding categorical columns\n",
    "class LabelEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X[col])\n",
    "            self.label_encoders[col] = le\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        for col, le in self.label_encoders.items():\n",
    "            # Handle unknown labels by ignoring them\n",
    "            X_encoded[col] = X[col].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
    "        return X_encoded\n",
    "\n",
    "\n",
    "# Step 1: Load datasets and create DELAYED column\n",
    "df_1991 = pd.read_csv('1991_cleaned.csv.gz').sample(10000)\n",
    "df_2001 = pd.read_csv('2001_cleaned.csv.gz').sample(10000)\n",
    "\n",
    "df_1991['DELAYED'] = (df_1991['ArrDelay'] > 0).astype(int)\n",
    "df_2001['DELAYED'] = (df_2001['ArrDelay'] > 0).astype(int)\n",
    "\n",
    "df_1991 = df_1991.drop('ArrDelay', axis=1)\n",
    "df_2001 = df_2001.drop('ArrDelay', axis=1)\n",
    "\n",
    "# Step 2: Feature engineering\n",
    "# Define numerical and categorical columns for each dataset\n",
    "num_cols_1991 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'DepDelay',\n",
    "                 'Cancelled', 'Diverted']\n",
    "cat_cols_1991 = [\"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
    "\n",
    "num_cols_2001 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'AirTime', 'DepDelay',\n",
    "                 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted']\n",
    "cat_cols_2001 = [\"UniqueCarrier\", \"TailNum\", \"Origin\", \"Dest\"]\n",
    "\n",
    "# Step 3: Create transformers for numerical and categorical columns\n",
    "num_transformer = StandardScaler()\n",
    "cat_transformer = LabelEncoderTransformer()  # Use custom LabelEncoderTransformer\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor_1991 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_1991),\n",
    "        ('cat', cat_transformer, cat_cols_1991)\n",
    "    ])\n",
    "\n",
    "preprocessor_2001 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_2001),\n",
    "        ('cat', cat_transformer, cat_cols_2001)\n",
    "    ])\n",
    "\n",
    "# Step 4: Train and evaluate ML models\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_1991, X_test_1991, y_train_1991, y_test_1991 = train_test_split(\n",
    "    df_1991.drop('DELAYED', axis=1), df_1991['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_2001, X_test_2001, y_train_2001, y_test_2001 = train_test_split(\n",
    "    df_2001.drop('DELAYED', axis=1), df_2001['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Create empty DataFrames for accuracies and feature importance\n",
    "accuracy_table_1991 = pd.DataFrame(index=['Accuracy'])\n",
    "feature_importance_table_1991 = pd.DataFrame()\n",
    "\n",
    "accuracy_table_2001 = pd.DataFrame(index=['Accuracy'])\n",
    "feature_importance_table_2001 = pd.DataFrame()\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model in models.items():\n",
    "    # Training and evaluating on 1991 data\n",
    "    model_pipe_1991 = Pipeline([('preprocessor', preprocessor_1991), ('model', model)])\n",
    "    accuracy_1991 = train_and_evaluate_model(model_pipe_1991, X_train_1991, y_train_1991, X_test_1991, y_test_1991)\n",
    "    accuracy_table_1991[model_name] = [f\"{accuracy_1991 * 100:.3f}%\"]\n",
    "\n",
    "    # Get feature importance for 1991\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance_1991 = model.feature_importances_\n",
    "        features_1991 = X_train_1991.columns\n",
    "        importance_dict_1991 = dict(zip(features_1991, feature_importance_1991))\n",
    "        feature_importance_table_1991[model_name] = pd.Series(importance_dict_1991).apply(lambda x: f\"{x * 100:.3f}%\")\n",
    "\n",
    "    # Training and evaluating on 2001 data\n",
    "    model_pipe_2001 = Pipeline([('preprocessor', preprocessor_2001), ('model', model)])\n",
    "    accuracy_2001 = train_and_evaluate_model(model_pipe_2001, X_train_2001, y_train_2001, X_test_2001, y_test_2001)\n",
    "    accuracy_table_2001[model_name] = [f\"{accuracy_2001 * 100:.3f}%\"]\n",
    "\n",
    "    # Get feature importance for 2001\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance_2001 = model.feature_importances_\n",
    "        features_2001 = X_train_2001.columns\n",
    "        importance_dict_2001 = dict(zip(features_2001, feature_importance_2001))\n",
    "        feature_importance_table_2001[model_name] = pd.Series(importance_dict_2001).apply(lambda x: f\"{x * 100:.3f}%\")\n",
    "\n",
    "# Display the tables\n",
    "print(\"Accuracy Table for 1991 Dataset:\")\n",
    "print(accuracy_table_1991)\n",
    "\n",
    "print(\"\\nAccuracy Table for 2001 Dataset:\")\n",
    "print(accuracy_table_2001)\n",
    "\n",
    "print(\"\\nFeature Importance Table for 1991 Dataset:\")\n",
    "print(feature_importance_table_1991)\n",
    "\n",
    "print(\"\\nFeature Importance Table for 2001 Dataset:\")\n",
    "print(feature_importance_table_2001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Step 1: Load datasets and create DELAYED column\n",
    "df_1991 = pd.read_csv('1991_cleaned.csv.gz')\n",
    "df_2001 = pd.read_csv('2001_cleaned.csv.gz')\n",
    "\n",
    "df_1991['DELAYED'] = (df_1991['ArrDelay'] > 0).astype(int)\n",
    "df_2001['DELAYED'] = (df_2001['ArrDelay'] > 0).astype(int)\n",
    "\n",
    "df_1991 = df_1991.drop('ArrDelay', axis=1)\n",
    "df_2001 = df_2001.drop('ArrDelay', axis=1)\n",
    "\n",
    "# Step 2: Feature engineering\n",
    "# Selecting relevant columns\n",
    "cat_cols = [\"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
    "num_cols_1991 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'DepDelay', 'Distance',\n",
    "                 'Cancelled', 'Diverted']\n",
    "num_cols_2001 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'AirTime', 'DepDelay',\n",
    "                 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted']\n",
    "\n",
    "# Step 3: Create transformers for numerical and categorical columns\n",
    "num_transformer = StandardScaler()\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor_1991 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_1991),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "preprocessor_2001 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_2001),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Step 4: Train and evaluate ML models\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_1991, X_test_1991, y_train_1991, y_test_1991 = train_test_split(\n",
    "    df_1991.drop('DELAYED', axis=1), df_1991['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_2001, X_test_2001, y_train_2001, y_test_2001 = train_test_split(\n",
    "    df_2001.drop('DELAYED', axis=1), df_2001['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Create empty DataFrames for accuracies\n",
    "accuracy_table_1991 = pd.DataFrame(index=['Accuracy'])\n",
    "accuracy_table_2001 = pd.DataFrame(index=['Accuracy'])\n",
    "\n",
    "# Load models and evaluate accuracies\n",
    "for model_name, model in models.items():\n",
    "    # Load model for 1991\n",
    "    model_filename_1991 = f\"models/{model_name}_1991_model.joblib\"\n",
    "    model_pipe_1991 = joblib.load(model_filename_1991)\n",
    "\n",
    "    # Make predictions on the test data for 1991\n",
    "    y_pred_1991 = model_pipe_1991.predict(X_test_1991)\n",
    "\n",
    "    # Evaluate accuracy for 1991\n",
    "    accuracy_1991 = accuracy_score(y_test_1991, y_pred_1991)\n",
    "    accuracy_table_1991[model_name] = [f\"{accuracy_1991 * 100:.3f}%\"]\n",
    "\n",
    "    # Load model for 2001\n",
    "    model_filename_2001 = f\"models/{model_name}_2001_model.joblib\"\n",
    "    model_pipe_2001 = joblib.load(model_filename_2001)\n",
    "\n",
    "    # Make predictions on the test data for 2001\n",
    "    y_pred_2001 = model_pipe_2001.predict(X_test_2001)\n",
    "\n",
    "    # Evaluate accuracy for 2001\n",
    "    accuracy_2001 = accuracy_score(y_test_2001, y_pred_2001)\n",
    "    accuracy_table_2001[model_name] = [f\"{accuracy_2001 * 100:.3f}%\"]\n",
    "\n",
    "# Save accuracy tables to CSV files\n",
    "accuracy_table_1991.to_csv(\"accuracy_table_1991.csv\")\n",
    "accuracy_table_2001.to_csv(\"accuracy_table_2001.csv\")\n",
    "\n",
    "# Display the accuracy tables\n",
    "print(\"Accuracy Table for 1991 Dataset:\")\n",
    "print(accuracy_table_1991)\n",
    "\n",
    "print(\"\\nAccuracy Table for 2001 Dataset:\")\n",
    "print(accuracy_table_2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df1991_FI_DT = pd.read_csv('feature_importance/Decision Tree_1991_feature_importance.csv')\n",
    "df1991_FI_XGB = pd.read_csv('feature_importance/XGBoost_1991_feature_importance.csv')\n",
    "\n",
    "# Merge DataFrames on the 'Feature' column\n",
    "df_combined = pd.merge(df1991_FI_DT, df1991_FI_XGB, on='Feature')\n",
    "\n",
    "# Set 'Feature' as the index and set index name to an empty string\n",
    "df_combined.set_index('Feature', inplace=True)\n",
    "# df_combined.index.name = ''\n",
    "\n",
    "# Rename columns to include model names\n",
    "df_combined = df_combined.rename(columns={'Importance_x': 'DT', 'Importance_y': 'XGBoost'})\n",
    "\n",
    "# Multiply values by 100 and format as percentages\n",
    "df_combined *= 100\n",
    "df_combined = df_combined.applymap(lambda x: f'{x:.2f}')\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(df_combined)\n",
    "\n",
    "# Create the 'feature_importance' subfolder if it doesn't exist\n",
    "os.makedirs('feature_importance', exist_ok=True)\n",
    "\n",
    "# Save the combined and formatted DataFrame to a new CSV file\n",
    "df_combined.to_csv('feature_importance/combined_feature_importance_1991.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Feature Importance for 1991 Dataset:\n",
      "DepDelay: 24.10%\n",
      "ActualElapsedTime: 16.75%\n",
      "Distance: 15.85%\n",
      "DepTime: 8.50%\n",
      "Origin: 7.65%\n",
      "Dest: 6.50%\n",
      "FlightNum: 6.27%\n",
      "Month: 5.81%\n",
      "UniqueCarrier: 3.83%\n",
      "DayofMonth: 3.02%\n",
      "DayOfWeek: 1.72%\n",
      "Cancelled: 0.00%\n",
      "Diverted: 0.00%\n",
      "\n",
      "Decision Tree Feature Importance for 2001 Dataset:\n",
      "DepDelay: 35.04%\n",
      "TaxiOut: 11.09%\n",
      "Distance: 9.61%\n",
      "ActualElapsedTime: 8.39%\n",
      "DepTime: 5.59%\n",
      "Origin: 4.88%\n",
      "FlightNum: 4.69%\n",
      "TaxiIn: 3.75%\n",
      "Dest: 3.69%\n",
      "AirTime: 3.44%\n",
      "Month: 3.31%\n",
      "DayofMonth: 2.86%\n",
      "UniqueCarrier: 2.05%\n",
      "DayOfWeek: 1.62%\n",
      "Cancelled: 0.00%\n",
      "Diverted: 0.00%\n",
      "\n",
      "XGBoost Feature Importance for 1991 Dataset:\n",
      "DepDelay: 54.37%\n",
      "Distance: 10.80%\n",
      "ActualElapsedTime: 9.65%\n",
      "Origin: 5.86%\n",
      "DayOfWeek: 4.76%\n",
      "UniqueCarrier: 4.55%\n",
      "Dest: 3.66%\n",
      "Month: 2.49%\n",
      "DepTime: 1.78%\n",
      "FlightNum: 1.61%\n",
      "DayofMonth: 0.47%\n",
      "Cancelled: 0.00%\n",
      "Diverted: 0.00%\n",
      "\n",
      "XGBoost Feature Importance for 2001 Dataset:\n",
      "DepDelay: 47.77%\n",
      "TaxiOut: 18.29%\n",
      "TaxiIn: 9.58%\n",
      "Distance: 3.95%\n",
      "AirTime: 3.83%\n",
      "ActualElapsedTime: 3.59%\n",
      "Origin: 2.90%\n",
      "UniqueCarrier: 2.70%\n",
      "Dest: 1.81%\n",
      "FlightNum: 1.66%\n",
      "DayOfWeek: 1.65%\n",
      "DepTime: 1.18%\n",
      "Month: 0.97%\n",
      "DayofMonth: 0.13%\n",
      "Cancelled: 0.00%\n",
      "Diverted: 0.00%\n",
      "\n",
      "Decision Tree Accuracy for 1991 Dataset: 90.38%\n",
      "Decision Tree Accuracy for 2001 Dataset: 86.34%\n",
      "\n",
      "XGBoost Accuracy for 1991 Dataset: 86.74%\n",
      "XGBoost Accuracy for 2001 Dataset: 89.18%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load datasets\n",
    "df_1991 = pd.read_csv('1991_cleaned.csv.gz')\n",
    "df_2001 = pd.read_csv('2001_cleaned.csv.gz')\n",
    "\n",
    "# Create the 'DELAYED' column\n",
    "df_1991['DELAYED'] = (df_1991['ArrDelay'] > 0).astype(int)\n",
    "df_2001['DELAYED'] = (df_2001['ArrDelay'] > 0).astype(int)\n",
    "\n",
    "# Drop the 'ArrDelay' column\n",
    "df_1991 = df_1991.drop('ArrDelay', axis=1)\n",
    "df_2001 = df_2001.drop('ArrDelay', axis=1)\n",
    "\n",
    "# Define numerical and categorical columns for each dataset\n",
    "num_cols_1991 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'DepDelay', 'Distance',\n",
    "                 'Cancelled', 'Diverted']\n",
    "num_cols_2001 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'AirTime', 'DepDelay', 'Distance',\n",
    "                 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted']\n",
    "cat_cols_1991 = [\"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
    "cat_cols_2001 = [\"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
    "\n",
    "# Separate the target variable\n",
    "target_variable = ['DELAYED']\n",
    "\n",
    "# Store a copy of the target variable separately\n",
    "df_1991_target = df_1991[target_variable].copy()\n",
    "df_2001_target = df_2001[target_variable].copy()\n",
    "\n",
    "# Drop the target variable for feature scaling\n",
    "df_1991_features = df_1991.drop(target_variable, axis=1)\n",
    "df_2001_features = df_2001.drop(target_variable, axis=1)\n",
    "\n",
    "# Apply label encoding to categorical variables using factorize\n",
    "for col in cat_cols_1991:\n",
    "    df_1991_features[col], _ = pd.factorize(df_1991_features[col])\n",
    "\n",
    "for col in cat_cols_2001:\n",
    "    df_2001_features[col], _ = pd.factorize(df_2001_features[col])\n",
    "\n",
    "# Concatenate numerical and categorical columns\n",
    "df_1991_combined = pd.concat([df_1991_features[num_cols_1991], df_1991_features[cat_cols_1991]], axis=1)\n",
    "df_2001_combined = pd.concat([df_2001_features[num_cols_2001], df_2001_features[cat_cols_2001]], axis=1)\n",
    "\n",
    "# Standardize combined numerical and categorical variables using the mean and standard deviation of each dataset\n",
    "scaler_1991 = StandardScaler()\n",
    "scaler_2001 = StandardScaler()\n",
    "\n",
    "df_1991_scaled = scaler_1991.fit_transform(df_1991_combined)\n",
    "df_1991_scaled = pd.DataFrame(df_1991_scaled, columns=df_1991_combined.columns)\n",
    "df_1991_scaled['DELAYED'] = df_1991_target.reset_index(drop=True)  # Reset index\n",
    "\n",
    "df_2001_scaled = scaler_2001.fit_transform(df_2001_combined)\n",
    "df_2001_scaled = pd.DataFrame(df_2001_scaled, columns=df_2001_combined.columns)\n",
    "df_2001_scaled['DELAYED'] = df_2001_target.reset_index(drop=True)  # Reset index\n",
    "\n",
    "os.makedirs(\"engineering\", exist_ok=True)\n",
    "pd.DataFrame(df_1991_scaled.sample(50)).to_csv('engineering/engineering_1991.csv', index=False)\n",
    "pd.DataFrame(df_2001_scaled.sample(50)).to_csv('engineering/engineering_2001.csv', index=False)\n",
    "\n",
    "# Split the 1991 dataset into training, testing, and validation sets\n",
    "train_data_1991, test_data_1991 = train_test_split(df_1991_scaled, test_size=0.2, random_state=42)\n",
    "train_data_1991, val_data_1991 = train_test_split(train_data_1991, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the 2001 dataset into training, testing, and validation sets\n",
    "train_data_2001, test_data_2001 = train_test_split(df_2001_scaled, test_size=0.2, random_state=42)\n",
    "train_data_2001, val_data_2001 = train_test_split(train_data_2001, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features and target for each dataset\n",
    "X_train_1991 = train_data_1991[num_cols_1991 + cat_cols_1991]\n",
    "y_train_1991 = train_data_1991['DELAYED']\n",
    "X_val_1991 = val_data_1991[num_cols_1991 + cat_cols_1991]\n",
    "y_val_1991 = val_data_1991['DELAYED']\n",
    "X_test_1991 = test_data_1991[num_cols_1991 + cat_cols_1991]\n",
    "y_test_1991 = test_data_1991['DELAYED']\n",
    "\n",
    "X_train_2001 = train_data_2001[num_cols_2001 + cat_cols_2001]\n",
    "y_train_2001 = train_data_2001['DELAYED']\n",
    "X_val_2001 = val_data_2001[num_cols_2001 + cat_cols_2001]\n",
    "y_val_2001 = val_data_2001['DELAYED']\n",
    "X_test_2001 = test_data_2001[num_cols_2001 + cat_cols_2001]\n",
    "y_test_2001 = test_data_2001['DELAYED']\n",
    "\n",
    "# Create folders if they do not exist\n",
    "folders = ['models', 'feature_importance', 'accuracies']\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Function to extract feature importance from tree-based models\n",
    "def get_tree_feature_importance(model, feature_names):\n",
    "    return dict(zip(feature_names, model.feature_importances_))\n",
    "\n",
    "# Define the Decision Tree model\n",
    "dt_model_1991 = DecisionTreeClassifier(random_state=42)\n",
    "dt_model_2001 = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the Decision Tree model to the 1991 dataset\n",
    "dt_model_1991.fit(X_train_1991, y_train_1991)\n",
    "\n",
    "# Fit the Decision Tree model to the 2001 dataset\n",
    "dt_model_2001.fit(X_train_2001, y_train_2001)\n",
    "\n",
    "# Extract feature importance for Decision Tree model (1991 dataset)\n",
    "feature_importance_dt_1991 = get_tree_feature_importance(dt_model_1991, num_cols_1991 + cat_cols_1991)\n",
    "\n",
    "# Extract feature importance for Decision Tree model (2001 dataset)\n",
    "feature_importance_dt_2001 = get_tree_feature_importance(dt_model_2001, num_cols_2001 + cat_cols_2001)\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model_1991 = XGBClassifier(random_state=42)\n",
    "xgb_model_2001 = XGBClassifier(random_state=42)\n",
    "\n",
    "# Fit the XGBoost model to the 1991 dataset\n",
    "xgb_model_1991.fit(X_train_1991, y_train_1991)\n",
    "\n",
    "# Fit the XGBoost model to the 2001 dataset\n",
    "xgb_model_2001.fit(X_train_2001, y_train_2001)\n",
    "\n",
    "# Extract feature importance for XGBoost model (1991 dataset)\n",
    "feature_importance_xgb_1991 = get_tree_feature_importance(xgb_model_1991, num_cols_1991 + cat_cols_1991)\n",
    "\n",
    "# Extract feature importance for XGBoost model (2001 dataset)\n",
    "feature_importance_xgb_2001 = get_tree_feature_importance(xgb_model_2001, num_cols_2001 + cat_cols_2001)\n",
    "\n",
    "# Print feature importance for Decision Tree model (1991 dataset)\n",
    "print(\"\\nDecision Tree Feature Importance for 1991 Dataset:\")\n",
    "for feature, importance in sorted(feature_importance_dt_1991.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{feature}: {importance*100:.2f}%\")\n",
    "\n",
    "# Print feature importance for Decision Tree model (2001 dataset)\n",
    "print(\"\\nDecision Tree Feature Importance for 2001 Dataset:\")\n",
    "for feature, importance in sorted(feature_importance_dt_2001.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{feature}: {importance*100:.2f}%\")\n",
    "\n",
    "# Print feature importance for XGBoost model (1991 dataset)\n",
    "print(\"\\nXGBoost Feature Importance for 1991 Dataset:\")\n",
    "for feature, importance in sorted(feature_importance_xgb_1991.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{feature}: {importance*100:.2f}%\")\n",
    "\n",
    "# Print feature importance for XGBoost model (2001 dataset)\n",
    "print(\"\\nXGBoost Feature Importance for 2001 Dataset:\")\n",
    "for feature, importance in sorted(feature_importance_xgb_2001.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{feature}: {importance*100:.2f}%\")\n",
    "\n",
    "# Save Decision Tree models\n",
    "joblib.dump(dt_model_1991, 'models/DT_1991_model.joblib')\n",
    "joblib.dump(dt_model_2001, 'models/DT_2001_model.joblib')\n",
    "\n",
    "# Save XGBoost models\n",
    "joblib.dump(xgb_model_1991, 'models/XGB_1991_model.joblib')\n",
    "joblib.dump(xgb_model_2001, 'models/XGB_2001_model.joblib')\n",
    "\n",
    "# Save feature importance as CSV\n",
    "feature_importance_dt_xgb_1991 = {'Feature': [], 'DT_Importance': [], 'XGB_Importance': []}\n",
    "feature_importance_dt_xgb_2001 = {'Feature': [], 'DT_Importance': [], 'XGB_Importance': []}\n",
    "\n",
    "for feature in num_cols_1991 + cat_cols_1991:\n",
    "    feature_importance_dt_xgb_1991['Feature'].append(feature)\n",
    "    feature_importance_dt_xgb_1991['DT_Importance'].append(feature_importance_dt_1991.get(feature, 0))\n",
    "    feature_importance_dt_xgb_1991['XGB_Importance'].append(feature_importance_xgb_1991.get(feature, 0))\n",
    "\n",
    "for feature in num_cols_2001 + cat_cols_2001:\n",
    "    feature_importance_dt_xgb_2001['Feature'].append(feature)\n",
    "    feature_importance_dt_xgb_2001['DT_Importance'].append(feature_importance_dt_2001.get(feature, 0))\n",
    "    feature_importance_dt_xgb_2001['XGB_Importance'].append(feature_importance_xgb_2001.get(feature, 0))\n",
    "\n",
    "df_feature_importance_1991 = pd.DataFrame(feature_importance_dt_xgb_1991)\n",
    "df_feature_importance_2001 = pd.DataFrame(feature_importance_dt_xgb_2001)\n",
    "\n",
    "df_feature_importance_1991.to_csv('feature_importance/feature_importance_1991.csv', index=False)\n",
    "df_feature_importance_2001.to_csv('feature_importance/feature_importance_2001.csv', index=False)\n",
    "\n",
    "predictions_dt_1991 = dt_model_1991.predict(X_test_1991)\n",
    "accuracy_dt_1991 = accuracy_score(y_test_1991, predictions_dt_1991)\n",
    "print(f\"\\nDecision Tree Accuracy for 1991 Dataset: {accuracy_dt_1991*100:.2f}%\")\n",
    "\n",
    "# Make predictions and evaluate for Decision Tree (2001 dataset)\n",
    "predictions_dt_2001 = dt_model_2001.predict(X_test_2001)\n",
    "accuracy_dt_2001 = accuracy_score(y_test_2001, predictions_dt_2001)\n",
    "print(f\"Decision Tree Accuracy for 2001 Dataset: {accuracy_dt_2001*100:.2f}%\")\n",
    "\n",
    "# Make predictions and evaluate for XGBoost (1991 dataset)\n",
    "predictions_xgb_1991 = xgb_model_1991.predict(X_test_1991)\n",
    "accuracy_xgb_1991 = accuracy_score(y_test_1991, predictions_xgb_1991)\n",
    "print(f\"\\nXGBoost Accuracy for 1991 Dataset: {accuracy_xgb_1991*100:.2f}%\")\n",
    "\n",
    "# Make predictions and evaluate for XGBoost (2001 dataset)\n",
    "predictions_xgb_2001 = xgb_model_2001.predict(X_test_2001)\n",
    "accuracy_xgb_2001 = accuracy_score(y_test_2001, predictions_xgb_2001)\n",
    "print(f\"XGBoost Accuracy for 2001 Dataset: {accuracy_xgb_2001*100:.2f}%\")\n",
    "\n",
    "# Save accuracies as CSV\n",
    "df_accuracies = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'XGBoost'],\n",
    "    '1991 Accuracy': [accuracy_dt_1991, accuracy_xgb_1991],\n",
    "    '2001 Accuracy': [accuracy_dt_2001, accuracy_xgb_2001]\n",
    "})\n",
    "\n",
    "df_accuracies.to_csv('accuracies/accuracies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
