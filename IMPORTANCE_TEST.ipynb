{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Random Forest on 1991 data:\n",
      "Random Forest Accuracy on 1991 data: 71.650%\n",
      "\n",
      "Training and evaluating Random Forest on 2001 data:\n",
      "Random Forest Accuracy on 2001 data: 80.450%\n",
      "\n",
      "Feature Importance for Random Forest on 1991 data:\n",
      "DepDelay: 35.21%\n",
      "Distance: 12.97%\n",
      "FlightNum: 7.36%\n",
      "DepTime: 6.97%\n",
      "ActualElapsedTime: 6.44%\n",
      "Origin: 6.39%\n",
      "UniqueCarrier: 6.03%\n",
      "Dest: 5.77%\n",
      "DayofMonth: 5.03%\n",
      "Month: 4.32%\n",
      "DayOfWeek: 3.50%\n",
      "Cancelled: 0.00%\n",
      "Diverted: 0.00%\n",
      "\n",
      "Feature Importance for Random Forest on 2001 data:\n",
      "ActualElapsedTime: 34.66%\n",
      "Origin: 12.77%\n",
      "FlightNum: 7.24%\n",
      "DepTime: 6.87%\n",
      "TailNum: 6.34%\n",
      "AirTime: 6.29%\n",
      "UniqueCarrier: 5.93%\n",
      "DepDelay: 5.68%\n",
      "DayofMonth: 4.95%\n",
      "Month: 4.25%\n",
      "DayOfWeek: 3.45%\n",
      "TaxiIn: 0.63%\n",
      "Cancelled: 0.42%\n",
      "Diverted: 0.41%\n",
      "TaxiOut: 0.10%\n",
      "Dest: 0.00%\n",
      "Distance: 0.00%\n",
      "\n",
      "Training and evaluating SVM on 1991 data:\n",
      "SVM Accuracy on 1991 data: 79.350%\n",
      "\n",
      "Training and evaluating SVM on 2001 data:\n",
      "SVM Accuracy on 2001 data: 83.350%\n",
      "\n",
      "Training and evaluating Decision Tree on 1991 data:\n",
      "Decision Tree Accuracy on 1991 data: 71.850%\n",
      "\n",
      "Training and evaluating Decision Tree on 2001 data:\n",
      "Decision Tree Accuracy on 2001 data: 72.800%\n",
      "\n",
      "Feature Importance for Decision Tree on 1991 data:\n",
      "DepDelay: 44.31%\n",
      "Distance: 15.29%\n",
      "Origin: 7.57%\n",
      "Dest: 6.10%\n",
      "UniqueCarrier: 5.18%\n",
      "DepTime: 5.07%\n",
      "FlightNum: 4.46%\n",
      "ActualElapsedTime: 3.37%\n",
      "DayofMonth: 3.25%\n",
      "Month: 2.99%\n",
      "DayOfWeek: 2.39%\n",
      "Cancelled: 0.00%\n",
      "Diverted: 0.00%\n",
      "\n",
      "Feature Importance for Decision Tree on 2001 data:\n",
      "ActualElapsedTime: 43.79%\n",
      "Origin: 15.11%\n",
      "AirTime: 7.48%\n",
      "DepDelay: 6.03%\n",
      "UniqueCarrier: 5.12%\n",
      "DepTime: 5.01%\n",
      "FlightNum: 4.41%\n",
      "TailNum: 3.33%\n",
      "DayofMonth: 3.21%\n",
      "Month: 2.95%\n",
      "DayOfWeek: 2.37%\n",
      "Cancelled: 0.47%\n",
      "TaxiIn: 0.37%\n",
      "TaxiOut: 0.20%\n",
      "Diverted: 0.13%\n",
      "Dest: 0.00%\n",
      "Distance: 0.00%\n",
      "\n",
      "Training and evaluating XGBoost on 1991 data:\n",
      "XGBoost Accuracy on 1991 data: 79.650%\n",
      "\n",
      "Training and evaluating XGBoost on 2001 data:\n",
      "XGBoost Accuracy on 2001 data: 83.550%\n",
      "\n",
      "Feature Importance for XGBoost on 1991 data:\n",
      "DepDelay: 43.10%\n",
      "Distance: 15.34%\n",
      "Dest: 7.79%\n",
      "Origin: 7.44%\n",
      "FlightNum: 6.05%\n",
      "ActualElapsedTime: 5.97%\n",
      "UniqueCarrier: 3.53%\n",
      "DepTime: 2.94%\n",
      "DayOfWeek: 2.78%\n",
      "Month: 2.74%\n",
      "DayofMonth: 2.32%\n",
      "Cancelled: 0.00%\n",
      "Diverted: 0.00%\n",
      "\n",
      "Feature Importance for XGBoost on 2001 data:\n",
      "ActualElapsedTime: 33.17%\n",
      "Origin: 11.81%\n",
      "TaxiOut: 8.35%\n",
      "Cancelled: 6.66%\n",
      "DepDelay: 6.00%\n",
      "AirTime: 5.72%\n",
      "FlightNum: 4.66%\n",
      "TailNum: 4.59%\n",
      "TaxiIn: 4.46%\n",
      "Diverted: 3.56%\n",
      "UniqueCarrier: 2.72%\n",
      "DepTime: 2.27%\n",
      "DayOfWeek: 2.14%\n",
      "Month: 2.11%\n",
      "DayofMonth: 1.79%\n",
      "Dest: 0.00%\n",
      "Distance: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load datasets and create DELAYED column\n",
    "df_1991 = pd.read_csv('1991_cleaned.csv.gz').sample(n=10000, random_state=42)\n",
    "df_2001 = pd.read_csv('2001_cleaned.csv.gz').sample(n=10000, random_state=42)\n",
    "\n",
    "df_1991['DELAYED'] = (df_1991['ArrDelay'] > 0).astype(int)\n",
    "df_2001['DELAYED'] = (df_2001['ArrDelay'] > 0).astype(int)\n",
    "\n",
    "df_1991 = df_1991.drop('ArrDelay', axis=1)\n",
    "df_2001 = df_2001.drop('ArrDelay', axis=1)\n",
    "\n",
    "# Step 2: Feature engineering\n",
    "# Selecting relevant columns\n",
    "cat_cols = [\"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
    "num_cols_1991 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'DepDelay', 'Distance',\n",
    "                 'Cancelled', 'Diverted']\n",
    "num_cols_2001 = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'FlightNum',\n",
    "                 'ActualElapsedTime', 'AirTime', 'DepDelay',\n",
    "                 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted']\n",
    "\n",
    "# Step 3: Create transformers for numerical and categorical columns\n",
    "num_transformer = StandardScaler()\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor_1991 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_1991),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "preprocessor_2001 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_2001),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Step 4: Train and evaluate ML models\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_1991, X_test_1991, y_train_1991, y_test_1991 = train_test_split(\n",
    "    df_1991.drop('DELAYED', axis=1), df_1991['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_2001, X_test_2001, y_train_2001, y_test_2001 = train_test_split(\n",
    "    df_2001.drop('DELAYED', axis=1), df_2001['DELAYED'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Loop through models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining and evaluating {model_name} on 1991 data:\")\n",
    "    model_pipe_1991 = make_pipeline(preprocessor_1991, model)\n",
    "    accuracy_1991 = train_and_evaluate_model(model_pipe_1991, X_train_1991, y_train_1991, X_test_1991, y_test_1991)\n",
    "    print(f\"{model_name} Accuracy on 1991 data: {accuracy_1991 * 100:.3f}%\")\n",
    "\n",
    "    print(f\"\\nTraining and evaluating {model_name} on 2001 data:\")\n",
    "    model_pipe_2001 = make_pipeline(preprocessor_2001, model)\n",
    "    accuracy_2001 = train_and_evaluate_model(model_pipe_2001, X_train_2001, y_train_2001, X_test_2001, y_test_2001)\n",
    "    print(f\"{model_name} Accuracy on 2001 data: {accuracy_2001 * 100:.3f}%\")\n",
    "\n",
    "    # Get feature importance for 1991\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance_1991 = model.feature_importances_\n",
    "        features_1991 = X_train_1991.columns\n",
    "        importance_dict_1991 = dict(zip(features_1991, feature_importance_1991))\n",
    "        print(f\"\\nFeature Importance for {model_name} on 1991 data:\")\n",
    "        total_importance_1991 = sum(importance_dict_1991.values())\n",
    "        for feature, importance in sorted(importance_dict_1991.items(), key=lambda x: x[1], reverse=True):\n",
    "            importance_percentage_1991 = (importance / total_importance_1991) * 100\n",
    "            print(f\"{feature}: {importance_percentage_1991:.2f}%\")\n",
    "\n",
    "    # Get feature importance for 2001\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance_2001 = model.feature_importances_\n",
    "        features_2001 = X_train_2001.columns\n",
    "        importance_dict_2001 = dict(zip(features_2001, feature_importance_2001))\n",
    "        print(f\"\\nFeature Importance for {model_name} on 2001 data:\")\n",
    "        total_importance_2001 = sum(importance_dict_2001.values())\n",
    "        for feature, importance in sorted(importance_dict_2001.items(), key=lambda x: x[1], reverse=True):\n",
    "            importance_percentage_2001 = (importance / total_importance_2001) * 100\n",
    "            print(f\"{feature}: {importance_percentage_2001:.2f}%\")\n",
    "\n",
    "# # Create empty DataFrames for accuracies and feature importance\n",
    "# accuracy_table_1991 = pd.DataFrame(index=['Accuracy'])\n",
    "# feature_importance_table_1991 = pd.DataFrame()\n",
    "\n",
    "# accuracy_table_2001 = pd.DataFrame(index=['Accuracy'])\n",
    "# feature_importance_table_2001 = pd.DataFrame()\n",
    "\n",
    "# # Loop through models\n",
    "# for model_name, model in models.items():\n",
    "#     # Training and evaluating on 1991 data\n",
    "#     model_pipe_1991 = make_pipeline(preprocessor_1991, model)\n",
    "#     accuracy_1991 = train_and_evaluate_model(model_pipe_1991, X_train_1991, y_train_1991, X_test_1991, y_test_1991)\n",
    "#     accuracy_table_1991[model_name] = [f\"{accuracy_1991 * 100:.3f}%\"]\n",
    "\n",
    "#     # Get feature importance for 1991\n",
    "#     if hasattr(model, 'feature_importances_'):\n",
    "#         feature_importance_1991 = model.feature_importances_\n",
    "#         features_1991 = X_train_1991.columns\n",
    "#         importance_dict_1991 = dict(zip(features_1991, feature_importance_1991))\n",
    "#         feature_importance_table_1991[model_name] = pd.Series(importance_dict_1991).apply(lambda x: f\"{x * 100:.3f}%\")\n",
    "\n",
    "#     # Training and evaluating on 2001 data\n",
    "#     model_pipe_2001 = make_pipeline(preprocessor_2001, model)\n",
    "#     accuracy_2001 = train_and_evaluate_model(model_pipe_2001, X_train_2001, y_train_2001, X_test_2001, y_test_2001)\n",
    "#     accuracy_table_2001[model_name] = [f\"{accuracy_2001 * 100:.3f}%\"]\n",
    "\n",
    "#     # Get feature importance for 2001\n",
    "#     if hasattr(model, 'feature_importances_'):\n",
    "#         feature_importance_2001 = model.feature_importances_\n",
    "#         features_2001 = X_train_2001.columns\n",
    "#         importance_dict_2001 = dict(zip(features_2001, feature_importance_2001))\n",
    "#         feature_importance_table_2001[model_name] = pd.Series(importance_dict_2001).apply(lambda x: f\"{x * 100:.3f}%\")\n",
    "\n",
    "# # Display the tables\n",
    "# print(\"Accuracy Table for 1991 Dataset:\")\n",
    "# print(accuracy_table_1991)\n",
    "\n",
    "# print(\"\\nAccuracy Table for 2001 Dataset:\")\n",
    "# print(accuracy_table_2001)\n",
    "\n",
    "# print(\"\\nFeature Importance Table for 1991 Dataset:\")\n",
    "# print(feature_importance_table_1991)\n",
    "\n",
    "# print(\"\\nFeature Importance Table for 2001 Dataset:\")\n",
    "# print(feature_importance_table_2001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
